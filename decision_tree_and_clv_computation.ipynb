{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gen.models import Models\n",
    "from data_gen.models_dict_v2 import model_dict\n",
    "from data_gen.generate_synthetic_df import generate_synthetic_df\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = np.random.seed(1)\n",
    "number_of_samples = 5000\n",
    "train_ratio, valiation_ratio, test_ratio = 0.6,0.2,0.2 #i.e. 0.6*number_of_samples for training, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(sample_number, seed):\n",
    "    # Create synthetic dataframe\n",
    "    df = generate_synthetic_df(sample_number, seed)\n",
    "\n",
    "    # Instantiate an object from the class \"Models\"\n",
    "    models = Models(model_dict)\n",
    "\n",
    "    # Calculate the cost and price\n",
    "    cost = models.calculate_cost(df)\n",
    "    pricing = models.calculate_pricing(df)\n",
    "\n",
    "    # Calculate the profit on the synthetic dataframe\n",
    "    df[\"profit\"] = models.calculate_profit(cost, pricing)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = generate_data(number_of_samples, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_one_column(column):\n",
    "    mean = np.mean(column)\n",
    "    standard_deviation = np.std(column)\n",
    "    return (column - mean)/standard_deviation\n",
    "\n",
    "def standardize(dataframe):\n",
    "    #Given a pandas dataframe, we standardize every column.\n",
    "    number_of_columns = len(dataframe.columns)\n",
    "    for j in range(number_of_columns):\n",
    "        dataframe.iloc[:,j] = standardize_one_column(dataframe.iloc[:,j])\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_X_Y(df):\n",
    "    X = df.loc[:, df.columns != 'profit']\n",
    "    Y = df[\"profit\"]\n",
    "\n",
    "    X.loc[X[\"MARITAL_STATUS\"]==\"Single\", \"MARITAL_STATUS\"] = 0\n",
    "    X.loc[X[\"MARITAL_STATUS\"]==\"Not_Single\", \"MARITAL_STATUS\"] = 0\n",
    "\n",
    "    X = X.astype(int)\n",
    "    Y = Y.astype(int)\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = obtain_X_Y(df)\n",
    "\n",
    "X_TRAIN = X.iloc[:int(train_ratio*number_of_samples),:]\n",
    "Y_TRAIN = Y.iloc[:int(train_ratio*number_of_samples)]\n",
    "X_VALIDATION = X.iloc[int(train_ratio*number_of_samples):int((train_ratio+valiation_ratio)*number_of_samples),:]\n",
    "Y_VALIDATION = Y.iloc[int(train_ratio*number_of_samples):int((train_ratio+valiation_ratio)*number_of_samples)]\n",
    "X_TEST = X.iloc[int((train_ratio+valiation_ratio)*number_of_samples):,:]\n",
    "Y_TEST = Y.iloc[int((train_ratio+valiation_ratio)*number_of_samples):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_hat):\n",
    "    return np.sum( np.square(y-y_hat) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "525128.6317843694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "min_error = 999999999999999999999\n",
    "best_depth = None\n",
    "best_regressor = None\n",
    "\n",
    "for k in range(1,15):\n",
    "    regressor = DecisionTreeRegressor(criterion=\"squared_error\", max_depth=k)\n",
    "    regressor = regressor.fit(X_TRAIN, Y_TRAIN)\n",
    "    Y_VALIDATION_HAT = regressor.predict(X_VALIDATION)\n",
    "    error = MSE(Y_VALIDATION, Y_VALIDATION_HAT)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_depth = k\n",
    "        best_regressor = regressor\n",
    "\n",
    "print(k)\n",
    "print(min_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561205.7140404767\n"
     ]
    }
   ],
   "source": [
    "Y_TEST_HAT = best_regressor.predict(X_TEST)\n",
    "error = MSE(Y_TEST_HAT, Y_TEST)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Get the Transition Matrix\n",
    "\n",
    "By a **state**, we mean a leaf in the best regressor trained above.\n",
    "Our first goal is to get a list of all the states and get the decision path (i.e. given X, what leaf does X fall into?).\n",
    "\n",
    "It turns out that sklearn has one id associated to each tree node. Our `state` will thus be a list of integers corresponding to these leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False  True  True False\n",
      "  True  True False False  True  True  True False False False  True  True\n",
      "  True False False  True  True False  True  True False False False False\n",
      "  True  True False  True  True False  True False  True  True False False\n",
      " False  True  True False  True  True False False  True  True False  True\n",
      "  True False False False  True  True False False False  True  True False\n",
      "  True  True False False  True  True False  True  True False False False\n",
      " False  True  True False  True  True False False  True  True False  True\n",
      "  True False False  True  True False False  True  True  True False False\n",
      " False False False False  True  True False  True  True False False  True\n",
      "  True False  True  True False False False  True  True False  True  True\n",
      " False False  True  True False  True  True False False False False  True\n",
      "  True False  True  True  True False False False  True  True False  True\n",
      "  True False False  True  True False  True  True False False False False\n",
      " False  True  True False  True  True False False  True  True  True False\n",
      " False False  True  True False  True  True  True False False False  True\n",
      "  True False False  True  True False  True  True  True False False False\n",
      " False False False False  True  True False  True  True False False  True\n",
      "  True  True False False False  True  True False  True  True False False\n",
      "  True  True False  True  True False False False False  True  True False\n",
      "  True  True False False  True  True False  True  True False False False\n",
      "  True  True False  True  True False False  True  True False  True  True\n",
      " False False False False False  True  True False  True  True False  True\n",
      "  True False False  True False  True  True False False  True  True False\n",
      "  True  True False False False False  True  True False  True  True False\n",
      " False  True  True False  True  True False  True  True False False False\n",
      " False False False  True  True False  True  True False False  True  True\n",
      " False  True  True False False False  True  True False  True  True False\n",
      " False  True  True False  True  True False False False False  True  True\n",
      " False  True  True False  True False  True  True False False False  True\n",
      "  True False  True  True False False  True  True False  True  True False\n",
      " False False False False  True  True False  True  True False  True False\n",
      "  True  True False False False  True  True False  True  True False False\n",
      "  True  True False  True  True False False False  True  True False False\n",
      "  True  True False  True  True False False False  True  True False  True\n",
      "  True False  True False  True  True False False False False False False\n",
      " False False  True  True False  True  True False False  True  True False\n",
      "  True  True False False False  True  True False  True  True False False\n",
      "  True  True False  True  True False False False False  True  True  True\n",
      " False False  True  True False  True  True False False False  True  True\n",
      "  True False False  True  True False  True  True False False False False\n",
      " False  True  True False  True  True False False  True  True  True False\n",
      " False False  True  True False  True  True False  True False  True  True\n",
      " False False False False  True  True False  True  True False  True  True\n",
      " False False False  True  True False  True  True False  True  True False\n",
      " False False False False False  True  True False  True  True False False\n",
      "  True  True False  True  True False False False  True  True False  True\n",
      "  True False False  True  True False  True  True False False False False\n",
      "  True  True False  True  True False  True False  True  True False False\n",
      "  True  True  True False False False False False  True  True False  True\n",
      "  True False False  True  True False  True  True False False False  True\n",
      "  True False  True  True False False  True  True False  True  True False\n",
      " False False False  True  True False  True  True False  True  True False\n",
      " False False  True  True False  True  True False  True False  True  True\n",
      " False False False False False False False  True  True False  True  True\n",
      " False False  True  True False  True  True False False False  True  True\n",
      " False  True  True  True False False False False  True  True False  True\n",
      "  True False False  True  True False  True  True False False False  True\n",
      "  True False  True  True False False  True  True False  True  True False\n",
      " False False False False  True  True  True False False  True  True False\n",
      "  True  True False False False  True  True False  True  True False False\n",
      "  True  True False  True  True False False False False  True  True False\n",
      "  True  True False False  True  True False  True  True False False False\n",
      "  True  True False  True  True False  True  True False False False False\n",
      " False False  True  True False  True  True False False  True  True  True\n",
      " False False False  True  True  True  True False False False False  True\n",
      "  True  True False False  True  True False  True  True False False False\n",
      "  True  True False  True  True False  True False  True  True False False\n",
      " False False  True False  True  True False False  True  True False  True\n",
      "  True False False False  True  True False  True  True  True False False\n",
      " False False  True  True False  True  True False  True  True False False\n",
      "  True  True  True]\n",
      "[  9  10  12  13  16  17  18  22  23  24  27  28  30  31  36  37  39  40\n",
      "  42  44  45  49  50  52  53  56  57  59  60  64  65  69  70  72  73  76\n",
      "  77  79  80  85  86  88  89  92  93  95  96  99 100 103 104 105 112 113\n",
      " 115 116 119 120 122 123 127 128 130 131 134 135 137 138 143 144 146 147\n",
      " 148 152 153 155 156 159 160 162 163 169 170 172 173 176 177 178 182 183\n",
      " 185 186 187 191 192 195 196 198 199 200 208 209 211 212 215 216 217 221\n",
      " 222 224 225 228 229 231 232 237 238 240 241 244 245 247 248 252 253 255\n",
      " 256 259 260 262 263 269 270 272 273 275 276 279 281 282 285 286 288 289\n",
      " 294 295 297 298 301 302 304 305 307 308 315 316 318 319 322 323 325 326\n",
      " 330 331 333 334 337 338 340 341 346 347 349 350 352 354 355 359 360 362\n",
      " 363 366 367 369 370 376 377 379 380 382 384 385 389 390 392 393 396 397\n",
      " 399 400 404 405 408 409 411 412 416 417 419 420 422 424 425 434 435 437\n",
      " 438 441 442 444 445 449 450 452 453 456 457 459 460 465 466 467 470 471\n",
      " 473 474 478 479 480 483 484 486 487 493 494 496 497 500 501 502 506 507\n",
      " 509 510 512 514 515 520 521 523 524 526 527 531 532 534 535 537 538 545\n",
      " 546 548 549 552 553 555 556 560 561 563 564 567 568 570 571 576 577 579\n",
      " 580 582 584 585 588 589 590 596 597 599 600 603 604 606 607 611 612 614\n",
      " 615 618 619 621 622 627 628 630 631 633 634 638 639 641 642 644 646 647\n",
      " 655 656 658 659 662 663 665 666 670 671 673 674 675 680 681 683 684 687\n",
      " 688 690 691 695 696 698 699 702 703 705 706 712 713 714 717 718 720 721\n",
      " 725 726 728 729 732 733 735 736 741 742 744 745 748 749 751 752 756 757\n",
      " 759 760 762 763 770 771 773 774 777 778 779 783 784 785 786 791 792 793\n",
      " 796 797 799 800 804 805 807 808 810 812 813 818 820 821 824 825 827 828\n",
      " 832 833 835 836 837 842 843 845 846 848 849 852 853 854]\n"
     ]
    }
   ],
   "source": [
    "def get_state(regressor):\n",
    "    #Source: https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "    #This method takes our regressor as input and returns an array of boolean plus an array of integers.\n",
    "    #The array of booleans will have the length same as the total number of nodes and will indicate if each is a leaf.\n",
    "    #The array of integers will have the same length as the total number of leaves and will correspond to the location of \"True\" in the boolean array.\n",
    "    \n",
    "    regressor_tree = regressor.tree_\n",
    "\n",
    "    n_nodes = regressor_tree.node_count\n",
    "    children_left = regressor_tree.children_left\n",
    "    children_right = regressor_tree.children_right\n",
    "\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "    while len(stack) > 0:\n",
    "        # `pop` ensures each node is only visited once\n",
    "        node_id, depth = stack.pop()\n",
    "        node_depth[node_id] = depth\n",
    "\n",
    "        # If the left and right child of a node is not the same we have a split\n",
    "        # node\n",
    "        is_split_node = children_left[node_id] != children_right[node_id]\n",
    "        # If a split node, append left and right children and depth to `stack`\n",
    "        # so we can loop through them\n",
    "        if is_split_node:\n",
    "            stack.append((children_left[node_id], depth + 1))\n",
    "            stack.append((children_right[node_id], depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "\n",
    "    return is_leaves, np.nonzero(is_leaves)[0]\n",
    "\n",
    "is_leaves, states = get_state(best_regressor)\n",
    "print(is_leaves)\n",
    "print(states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.45000000e+01, -1.23333333e+02, -1.91500000e+02, -1.24000000e+02,\n",
       "       -1.47000000e+02, -1.58000000e+02, -1.86000000e+02, -9.43800000e+01,\n",
       "       -1.24333333e+02, -1.55000000e+02, -1.52500000e+02, -1.10000000e+02,\n",
       "       -1.06200000e+02, -1.43500000e+02, -6.89615385e+01, -5.30909091e+01,\n",
       "       -7.80000000e+01, -1.05666667e+02, -1.55000000e+02, -6.45000000e+01,\n",
       "       -1.31000000e+02, -9.40000000e+01, -1.34600000e+02, -1.70333333e+02,\n",
       "       -1.21000000e+02, -8.74814815e+01, -6.21250000e+01, -7.65000000e+01,\n",
       "       -1.33333333e+02, -1.63000000e+02, -9.00000000e+01, -1.15000000e+02,\n",
       "       -1.54500000e+02, -2.22000000e+02, -1.75600000e+02, -2.34750000e+02,\n",
       "       -2.90000000e+02, -2.11000000e+02, -1.79000000e+02, -1.26454545e+02,\n",
       "       -2.09000000e+02, -2.02500000e+02, -1.50750000e+02, -8.15000000e+01,\n",
       "       -1.03000000e+02, -1.52000000e+02, -1.06000000e+02, -2.69000000e+02,\n",
       "       -2.27000000e+02, -1.62000000e+02, -1.69000000e+02, -1.80000000e+02,\n",
       "       -3.02500000e+01, -8.20000000e+01, -8.00000000e+01, -7.50000000e+01,\n",
       "       -8.17619048e+01, -4.68333333e+01, -8.32857143e+01, -1.14333333e+02,\n",
       "       -1.10000000e+01, -3.46666667e+01, -7.00000000e+00, -8.00000000e+00,\n",
       "       -4.55000000e+01, -7.86666667e+01, -6.07500000e+01, -3.60000000e+01,\n",
       "       -1.74000000e+02, -1.71000000e+02, -1.10000000e+02, -1.37500000e+02,\n",
       "       -2.32000000e+02, -6.70000000e+01, -4.00000000e+01, -1.27666667e+02,\n",
       "       -8.25000000e+01, -1.20333333e+02, -8.65000000e+01, -1.68000000e+02,\n",
       "       -1.53000000e+02, -1.39000000e+02, -1.12000000e+02, -4.92500000e+01,\n",
       "       -9.35000000e+01, -2.44210526e+01, -6.60000000e+01, -9.20000000e+01,\n",
       "       -5.25000000e+00,  1.25000000e+01, -4.00000000e+01, -9.00000000e+00,\n",
       "       -7.90000000e+01, -6.20000000e+01, -5.90000000e+01, -1.42000000e+02,\n",
       "       -1.26000000e+02, -9.65000000e+01, -1.17000000e+02,  6.00000000e+00,\n",
       "       -4.75652174e+01, -6.71000000e+01, -1.03500000e+02, -7.56666667e+01,\n",
       "       -1.13000000e+02, -7.63333333e+01, -1.78000000e+02, -4.94000000e+01,\n",
       "       -3.00281690e+01, -7.60000000e+01, -3.20000000e+01, -6.73000000e+01,\n",
       "       -4.65312500e+01, -6.60000000e+01, -1.14666667e+02, -2.33833333e+01,\n",
       "       -6.07777778e+01, -6.43500000e+01, -1.87142857e+01, -5.64516129e+01,\n",
       "       -2.68000000e+01, -6.80000000e+01, -1.06333333e+02, -4.35897436e+00,\n",
       "       -2.04268293e+01, -2.59259259e-01, -3.03333333e+01, -6.64285714e+01,\n",
       "       -3.24705882e+01, -6.33333333e+00, -2.65555556e+01, -1.03500000e+02,\n",
       "       -1.43000000e+02, -1.54000000e+02, -1.19800000e+02, -2.73000000e+02,\n",
       "       -1.47000000e+02, -3.90000000e+01, -6.73333333e+01, -8.20000000e+01,\n",
       "       -1.33333333e+02, -7.90000000e+01, -1.17000000e+02, -8.48750000e+01,\n",
       "       -5.20000000e+01, -9.21666667e+01, -5.10000000e+01, -5.90000000e+01,\n",
       "       -2.60000000e+01, -2.20000000e+01, -7.53333333e+01, -4.52000000e+01,\n",
       "       -9.20000000e+01, -1.26000000e+02, -3.60000000e+01, -8.00000000e+00,\n",
       "       -2.90000000e+01, -6.13333333e+01, -6.26666667e+00,  1.06666667e+01,\n",
       "       -2.02777778e+01, -5.28571429e+00, -2.00000000e+00,  2.20000000e+01,\n",
       "        1.82222222e+01,  3.01000000e+01,  3.79310345e+00, -1.15789474e+01,\n",
       "        2.33333333e+00,  1.46666667e+01, -4.55000000e+01, -6.75000000e+01,\n",
       "       -2.40833333e+01, -6.40000000e+01, -1.14000000e+02, -4.15000000e+01,\n",
       "       -7.73750000e+01, -3.50000000e+01,  1.01111111e+01, -1.52500000e+01,\n",
       "       -3.40000000e+01,  0.00000000e+00, -1.70000000e+01, -6.00000000e+01,\n",
       "       -3.14444444e+01,  2.70000000e+01,  1.69375000e+01,  4.70000000e+01,\n",
       "        3.80000000e+01, -3.30000000e+01,  1.50000000e+01, -3.00000000e+00,\n",
       "        3.20000000e+01, -6.00000000e+00, -1.50000000e+01,  2.03000000e+01,\n",
       "        4.38461538e+01,  3.23333333e+01,  7.20000000e+01,  6.20000000e+01,\n",
       "       -7.00000000e+01, -5.10000000e+01, -4.20000000e+01, -1.66666667e+00,\n",
       "       -4.60000000e+01, -3.30000000e+01,  1.86250000e+01,  1.33333333e+00,\n",
       "        1.00000000e+01,  3.42000000e+01, -3.80000000e+01,  5.50000000e+00,\n",
       "       -2.10000000e+01, -4.00000000e-01,  1.18571429e+01,  2.80000000e+01,\n",
       "        2.10000000e+01, -3.00000000e+00,  1.16000000e+01, -1.26666667e+01,\n",
       "       -4.50000000e+01,  8.50000000e+00,  2.04210526e+01,  3.05918367e+01,\n",
       "        2.14583333e+01, -1.02500000e+01, -3.80000000e+01,  3.58333333e+00,\n",
       "        1.98571429e+01, -5.51250000e+01, -3.33333333e+01, -9.10000000e+01,\n",
       "       -1.06250000e+01, -3.30000000e+01, -1.60000000e+01,  1.66666667e-01,\n",
       "       -7.59090909e+00,  5.32000000e+00, -7.70000000e+01, -5.55000000e+01,\n",
       "       -3.10000000e+01, -3.30000000e+01, -3.00000000e+01,  2.02500000e+01,\n",
       "        3.50000000e+01,  2.12000000e+01, -1.00000000e+01,  3.10769231e+01,\n",
       "        4.04324324e+01,  9.00000000e+00,  7.50000000e+00, -7.25000000e+00,\n",
       "        3.18000000e+01,  1.62727273e+01, -4.80000000e+01, -2.30000000e+01,\n",
       "       -3.50000000e+01,  3.10000000e+01,  4.27826087e+01,  5.68000000e+01,\n",
       "        4.52500000e+01,  2.30000000e+01,  1.00000000e+01,  6.31250000e+01,\n",
       "        5.60000000e+01,  5.10000000e+01,  6.13333333e+01,  7.50000000e+01,\n",
       "        8.50000000e+01,  3.38461538e+01,  1.97333333e+01,  2.20000000e+01,\n",
       "       -5.75000000e+00,  4.46363636e+01,  3.43928571e+01,  2.48333333e+01,\n",
       "        1.53333333e+01,  4.88421053e+01,  5.35000000e+01,  5.70000000e+01,\n",
       "        3.25000000e+01,  2.71666667e+01,  3.45000000e+01,  3.50000000e+01,\n",
       "        4.83333333e+01, -3.30000000e+01, -3.80000000e+01, -2.20000000e+01,\n",
       "       -2.80000000e+01, -1.20000000e+01,  2.00000000e+00,  0.00000000e+00,\n",
       "        1.30000000e+01,  8.00000000e+00,  3.10000000e+01,  5.30000000e+01,\n",
       "        4.48500000e+01,  3.21111111e+01,  3.93000000e+01,  5.43000000e+01,\n",
       "        5.84761905e+01,  4.73750000e+01,  5.38333333e+01,  5.35384615e+01,\n",
       "        4.40000000e+01,  5.88148148e+01,  6.45333333e+01,  6.10000000e+01,\n",
       "        6.75000000e+01,  7.35000000e+01,  7.80000000e+01,  3.41428571e+01,\n",
       "        2.45833333e+01,  5.10000000e+01,  4.50000000e+01,  3.00000000e+00,\n",
       "       -2.00000000e+00,  4.80000000e+01,  4.40000000e+01,  5.00000000e+01,\n",
       "        5.75000000e+01,  1.40000000e+01,  4.03333333e+01,  1.80000000e+01,\n",
       "        5.65555556e+01,  6.68750000e+01,  5.26666667e+01,  4.68000000e+01,\n",
       "        4.80000000e+01,  3.05000000e+01,  4.34000000e+01,  5.14000000e+01,\n",
       "        6.48666667e+01,  7.07391304e+01,  6.30000000e+01,  5.33333333e+01,\n",
       "        2.40000000e+01,  5.95000000e+01,  6.66153846e+01,  6.11428571e+01,\n",
       "        5.52307692e+01,  7.45526316e+01,  6.83333333e+01,  6.60000000e+01,\n",
       "        5.20000000e+01,  8.12000000e+01,  8.70000000e+01,  7.43333333e+01,\n",
       "        7.78000000e+01,  8.60000000e+01,  8.46666667e+01,  7.80000000e+01,\n",
       "        8.26666667e+01,  8.40000000e+01,  8.65000000e+01,  7.30000000e+01,\n",
       "        9.12857143e+01,  8.50000000e+01,  8.45000000e+01,  8.74285714e+01,\n",
       "        7.79333333e+01,  5.90000000e+01,  5.80000000e+01,  6.00000000e+01,\n",
       "        7.72500000e+01,  8.17000000e+01,  8.63750000e+01,  8.20000000e+01,\n",
       "        8.91818182e+01,  9.11250000e+01,  9.28666667e+01,  8.80000000e+01,\n",
       "        9.60000000e+01,  9.50000000e+01,  9.90000000e+01,  9.70000000e+01,\n",
       "        8.26000000e+01,  7.50000000e+01,  9.40000000e+01,  8.45000000e+01,\n",
       "        9.10000000e+01,  9.70000000e+01,  1.00250000e+02,  9.74285714e+01,\n",
       "        1.02714286e+02,  9.87500000e+01,  9.25000000e+01,  8.90000000e+01,\n",
       "        9.60000000e+01,  1.06000000e+02,  1.01000000e+02,  1.01000000e+02,\n",
       "        1.09000000e+02,  1.12000000e+02,  1.14333333e+02,  1.09000000e+02,\n",
       "        1.04000000e+02,  1.06000000e+02,  1.13000000e+02,  1.07230769e+02,\n",
       "        1.17750000e+02,  1.21500000e+02,  1.10714286e+02,  1.15000000e+02,\n",
       "        1.26000000e+02,  1.21000000e+02,  1.23000000e+02,  1.39000000e+02,\n",
       "        1.25000000e+02,  1.20000000e+02,  1.32000000e+02,  1.28000000e+02,\n",
       "        1.38500000e+02,  1.47000000e+02,  1.73000000e+02,  1.54000000e+02,\n",
       "        1.41333333e+02,  1.56166667e+02,  1.20000000e+02,  1.46000000e+02,\n",
       "        1.31800000e+02,  1.22000000e+02,  1.19000000e+02,  1.57000000e+02,\n",
       "        1.46000000e+02,  1.05000000e+02,  1.08000000e+02,  1.18000000e+02])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_profit_list(regressor):\n",
    "    #this method returns a list such that, the profit of state i is list[i].\n",
    "    _, states = get_state(best_regressor)\n",
    "    value_list = np.array(regressor.tree_.value[:,0,0], dtype = np.float64)\n",
    "    return value_list[states]\n",
    "\n",
    "get_profit_list(best_regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us print, say, the decision path for `X_TEST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([245,  66, 342, 169, 221, 305, 302, 324, 286, 365, 145, 269, 115,\n",
       "       160,  42, 355, 217, 186, 272, 108,  14, 101, 245, 324, 169, 338,\n",
       "       386, 119,  90,   0,   7, 376, 305, 229, 128, 126,  14, 108, 117,\n",
       "       125, 119, 100,  57, 245, 112, 274, 131, 270, 115, 185, 368, 270,\n",
       "       338, 341, 236, 306, 169, 382, 123, 302, 111,  25, 376, 195,  61,\n",
       "       265, 253, 157, 151,  15, 192, 160, 237, 221, 311, 115, 417, 146,\n",
       "       168, 403, 338, 372, 123, 227, 125, 166, 370, 162, 300, 348, 273,\n",
       "        97, 217,   2,  25, 224, 342, 312, 229, 302, 115, 100, 128, 166,\n",
       "       245, 156, 224, 277, 295, 166, 186, 236, 417, 245, 125, 221, 270,\n",
       "       100, 354, 312, 322, 258, 305, 123, 124,  56, 108,  12,  16, 224,\n",
       "       270, 192, 338, 360, 347, 167, 237,  33, 186, 383,   7, 244, 341,\n",
       "       272, 112, 186, 124, 399, 112, 413,  27, 169, 297,  25, 364,  57,\n",
       "       233, 296, 128, 119, 217, 342,  25, 115, 224, 332, 115, 132, 372,\n",
       "       274, 192, 344,  57, 300, 337, 385, 124, 345, 224, 252, 123, 342,\n",
       "       102, 296,   8, 206, 164, 275, 416, 403, 245, 338, 217,  65, 229,\n",
       "       217, 282, 175,   7, 172, 192, 167, 223, 365, 192, 286, 197,  34,\n",
       "       169, 274,  86, 108, 125, 344, 330, 320, 298, 244,  45, 123, 415,\n",
       "        45, 274, 282, 101, 305, 248, 217, 338, 101, 333, 369, 403, 312,\n",
       "       115, 108, 108,  85, 258, 338, 186, 162, 196, 341, 326, 252, 314,\n",
       "       379, 274, 301, 230, 263, 116, 330, 300, 124, 338, 169, 274, 217,\n",
       "        15, 125, 124, 338, 224, 195, 273, 169, 217, 385, 184, 190, 253,\n",
       "       360,  14, 338,  26, 214, 384, 282,  87, 218, 353, 184, 248, 176,\n",
       "       127, 416, 298, 115, 337, 214, 169, 338, 112, 134, 115,  85,  85,\n",
       "       338, 275, 158, 128, 115, 115,  85, 115,   7, 119, 244, 348,  18,\n",
       "       304, 305, 333, 399, 217, 108, 124,  45,  17, 184, 403,   1, 327,\n",
       "       330, 223, 168, 278, 305, 101, 342, 269, 162, 337, 222,  88, 171,\n",
       "       115, 228, 208, 190, 140, 304, 248, 230, 224, 223, 344, 325, 285,\n",
       "       339,  25, 224, 186,  35,  60,  25, 342, 333, 169, 275, 334, 192,\n",
       "       169, 121, 281, 341, 341, 366, 355, 164, 366, 124, 177,  56, 172,\n",
       "       159, 133, 338, 123, 338, 306,  12, 160, 125, 100, 186,  79, 277,\n",
       "       326,  18, 247, 222, 186,  57, 370, 186, 217,   7,  44, 223, 101,\n",
       "       353, 168, 101, 124, 166, 108, 237, 327,  75, 274, 223, 250, 326,\n",
       "       333, 206,  44, 368,  88, 159,  25, 328, 237, 414, 122, 192, 169,\n",
       "       223, 124, 217, 115,  19, 115, 176,  25, 306, 235, 124, 224, 366,\n",
       "       368, 223, 319, 108, 263, 115, 214, 253, 224,  15, 108, 306, 115,\n",
       "       166, 167, 221, 228, 337, 207, 272, 184, 112, 151, 322, 360, 342,\n",
       "       115, 108, 301, 165, 305, 305, 111, 124,  25, 223,  57, 115, 160,\n",
       "       120, 400, 131, 119, 186, 108,  88, 302, 403, 214, 160,  85, 217,\n",
       "       359, 248, 115, 170, 115, 369, 175, 277, 125, 244,  65, 245, 115,\n",
       "       224, 130, 124, 158, 195, 419, 348, 342, 424, 184, 274, 330, 369,\n",
       "       270, 228, 322, 115, 228, 108, 186, 125,  98, 129,  99, 158, 115,\n",
       "       298, 300, 119, 169, 217,   7, 214,  34,  85, 353, 158, 337, 245,\n",
       "       359, 414, 399, 298, 370,  25,   7, 217, 123,  64, 145, 341,  25,\n",
       "       214, 340, 108,  68, 108, 348,  29, 119, 179, 123,  59,  32, 305,\n",
       "        56, 282, 124, 217,   3, 223, 124, 354, 410, 192, 422,  83,  85,\n",
       "       115,  80, 403, 341, 364, 124, 100, 160, 207, 351, 323, 124, 399,\n",
       "       258, 169, 214, 214, 257, 258, 367, 115, 124, 319, 111, 369,  54,\n",
       "       161, 164,  14,  67, 158, 115,  17, 292, 258, 154, 186, 145, 165,\n",
       "       337, 124, 269, 178, 302, 306, 186, 248, 277, 300, 101, 338, 305,\n",
       "       165, 373, 376, 304,  25,  22, 224, 192, 274, 270, 300, 125, 306,\n",
       "       353,  64, 192, 227, 331, 213, 408,  25,  14, 338, 248, 124, 123,\n",
       "       258, 256, 304, 355, 124,  45, 374, 296, 165,  65, 119, 115,  45,\n",
       "       410, 237, 119, 269, 216, 244, 301, 112, 235, 342, 217, 115,  79,\n",
       "       236, 327, 196,  66, 410,  85, 128, 233, 384,  57, 340,   1, 169,\n",
       "       100, 140, 126, 274, 338, 115,  59, 390,  18, 164, 368, 344,  14,\n",
       "       338, 341, 301,  38, 365, 128, 133, 395, 118, 124, 333,  25, 158,\n",
       "       166, 312, 253, 165, 330, 123, 117, 297, 360, 305, 274, 167, 124,\n",
       "       115, 366, 130, 115, 125, 248, 125, 324, 338, 108, 277, 338, 225,\n",
       "       339, 263, 115, 223, 162, 115, 201, 272, 322, 164,   7, 111, 333,\n",
       "       115, 270, 364, 192, 115, 116, 125,  25, 277, 124, 186, 195,  48,\n",
       "       167,  83, 322, 124, 324, 169, 360, 124, 101, 342, 389, 159, 400,\n",
       "       100, 217, 115, 247, 401, 341, 108, 298, 125, 108, 101,  85, 338,\n",
       "       124, 365,  32, 342, 338, 330, 342, 410, 307, 286, 278, 115, 282,\n",
       "       378, 252,  14, 332, 245, 115, 400,  34,  21, 169, 125, 162, 158,\n",
       "       324, 269, 423, 297, 259, 194, 217, 247, 340, 252, 292, 399, 115,\n",
       "       101, 297, 345, 304, 327, 115,  15, 217, 399, 410, 274, 123, 125,\n",
       "       162, 214, 274,  59, 378, 386, 161, 233,  47, 210, 248, 375, 273,\n",
       "       273, 341, 115, 176, 213, 342, 214, 221,  68, 197, 237, 196, 325,\n",
       "       229,  85, 258, 115, 169, 332,  46, 112, 383, 162, 333, 247, 337,\n",
       "       115, 325, 124,  80, 355,  10, 186,  56, 164, 338, 169, 278, 176,\n",
       "       257, 359, 115, 107, 415, 312, 115, 367, 338, 372, 305, 102, 185,\n",
       "       108, 269,  29, 209,  84, 167, 168, 299, 162, 124, 125, 104,  24,\n",
       "       213, 214, 159, 119, 368, 300, 299, 145, 297,  85, 224,  10, 422,\n",
       "       399, 111, 295, 221, 186, 244, 103, 307, 222, 376, 184, 224,  17,\n",
       "       108, 342, 245, 115, 125, 416, 164, 305, 305, 119, 214, 184, 114,\n",
       "       128, 124, 108, 193,  25, 282, 217, 231, 262, 258, 172, 273, 159,\n",
       "       281,  25, 322, 324,  64, 252, 101, 169, 273, 420, 273, 124],\n",
       "      dtype=uint16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_state_of_one_year(x, regressor, states):\n",
    "    x_leaf_id = regressor.apply(x)\n",
    "    x_states = np.empty(len(x), dtype = np.uint16)\n",
    "    for k in range(len(x)):\n",
    "        x_states[k] = np.where(states == x_leaf_id[k])[0][0]\n",
    "    return x_states\n",
    "\n",
    "get_state_of_one_year(X_TEST, best_regressor, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is random, we will use different seeds to generate $k$ pieces of data of same amound of clients. We assume that such is how one client changes over $k$ years. We shall focus on the mechanism of getting the transition probabilities and will ignore the fact that the client does not age by exactly one year old in the next year.\n",
    "\n",
    "We remark that, in order for the calculation to work, we must have at least one client for every state. This might not hold. Therefore, we use a large group of clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250 304 223 ... 266  25  89]\n"
     ]
    }
   ],
   "source": [
    "k = 50 #number of years\n",
    "seeds = np.random.choice(1000, k)\n",
    "#yearly_X = []\n",
    "yearly_state = []\n",
    "sample_number = 5000\n",
    "\n",
    "for i in range(k):\n",
    "    new_dataframe = generate_data(sample_number, seeds[i])\n",
    "    x,_ = obtain_X_Y(new_dataframe)\n",
    "    #yearly_X.append(x)\n",
    "    yearly_state.append(get_state_of_one_year(x,best_regressor, states))\n",
    "\n",
    "print(yearly_state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00334448 0.00167224 0.00167224 ... 0.         0.         0.00167224]\n",
      " [0.00684932 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00576369 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def compute_transition_matrix(yearly_state, number_of_states):\n",
    "    number_of_years = len(yearly_state)\n",
    "    number_of_clients = len(yearly_state[0])\n",
    "    transition_matrix = np.zeros((number_of_states,number_of_states), dtype = np.uint16)\n",
    "\n",
    "    #For each time t, we count the number of clients starting at state l and end at every state.\n",
    "    #Once this is done, we make each row a probability vector.\n",
    "\n",
    "    for t in range(number_of_years-1):\n",
    "        for n in range(number_of_clients):\n",
    "            state_this_year = int(yearly_state[t][n])\n",
    "            state_next_year = int(yearly_state[t+1][n])\n",
    "            transition_matrix[state_this_year,state_next_year] += 1\n",
    "    \n",
    "    transition_matrix = transition_matrix.astype(np.float64)\n",
    "    for l in range(number_of_states):\n",
    "        row_sum = np.sum(transition_matrix[l])\n",
    "        if row_sum == 0:\n",
    "            transition_matrix[l] = (1/number_of_states)*np.ones(len(transition_matrix[l]))\n",
    "        else:\n",
    "            transition_matrix[l] = transition_matrix[l]/row_sum\n",
    "    \n",
    "    return transition_matrix\n",
    "\n",
    "transition_matrix = compute_transition_matrix(yearly_state, len(states))\n",
    "print(transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Computation of Customer Lifetime Value(CLV)\n",
    "\n",
    "Fix a client of Intact.\n",
    "We are now in a position of defining the concept of customer lifetime value and introduce our algorithm to compute it.\n",
    "\n",
    "Let $A$ with some $\\sigma$-algebra be a measurable space, called the **state space**. For each $t \\in \\{0,1,2,\\cdots\\}$, the **client state** is an $A$-valued random variable $S_t$, all defined on one common probability space $(\\Omega, F, P)$. For fixed bounded measurable **profit function** $f: A \\to \\mathbb{R}$ and a **discounting factor** $\\gamma = \\frac{1}{1.15}$, we define, for every non-negative integer $t_0$ and every $a \\in A$:\n",
    "\n",
    "\\begin{equation*}\n",
    "CLV_{t_0}(a) = \\mathbb{E}[\\sum_{t=t_0 + 1}^{\\tau} \\gamma^t f(S_t) \\mid S_{t_0} = a]\n",
    "\\end{equation*}\n",
    "\n",
    "Here $\\tau$ is a positive finite stopping time indicating the time which client first quits using Intact. Our goal is to compute $CLV_0(a)$ for every $a \\in A$. \n",
    "\n",
    "[//]: <1. Enlarge the transition matrix with one row at bottom and one column at right. The bottom right corner of the matrix is $1$. Fill the last row with $0$ and the last column with $0.15$. Normalize the matrix so that it remains a transition matrix. The new state we have added represents the probability of client quiting Intact.>\n",
    "\n",
    "\n",
    "Let $a \\in A$ be given. For every $a' \\in A$, we have:\n",
    "\n",
    "\\begin{equation*}\n",
    "P\\{S_1=a' \\mid S_0=a\\} = P\\{S_1=a' \\mid S_0=a, \\text{client remains}\\} P\\{ \\text{client remains} \\mid S_0=a\\} \n",
    "\\end{equation*}\n",
    "\n",
    "The term $P\\{S_1=a' \\mid S_0=a, \\text{client remains}\\}$ on the right hand side is taken care of by the transition matrix computed above. We take $P\\{ \\text{client remains} \\mid S_0=a\\} = 0.15$. Generate standard uniform $V$. If $V < P\\{ \\text{client remains} \\mid S_0=a\\}$, then the client quits. Otherwise, generate independent standard uniform $U$, which looks at the transition matrix and decide the value of $S_1$. Had simulated the state of $S_t$, repeat in this manner to generate the state of $S_{t+1}$. In doing so, we have generating a sample for the integrand. We then compute the expectation using Kolmogorov's strong law of large numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_paths(number_of_paths, transition_matrix, initial_state, years_limit = 100, client_quit_rate = 0.15):\n",
    "    sample_paths = []\n",
    "    for _ in range(number_of_paths):\n",
    "        sample_paths.append(generate_one_sample_path(transition_matrix, initial_state, years_limit, client_quit_rate))\n",
    "    return sample_paths\n",
    "\n",
    "def generate_one_sample_path(transition_matrix, initial_state, years_limit = 100, client_quit_rate = 0.15):\n",
    "    number_of_states = transition_matrix.shape[0]\n",
    "    assert initial_state < number_of_states, \"State cannot be larger than dimension of matrix.\"\n",
    "    sample_path = [initial_state]\n",
    "    while True:\n",
    "        v = np.random.uniform(low=0.0, high=1.0, size=1)[0]\n",
    "        if v < client_quit_rate:\n",
    "            sample_path.append(-1)\n",
    "            return sample_path\n",
    "        last_state = sample_path[-1]\n",
    "        next_state = np.random.choice([i for i in range(number_of_states)], p=transition_matrix[last_state])\n",
    "        sample_path.append(next_state)\n",
    "        if len(sample_path) > years_limit:\n",
    "            return sample_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLV_one_path(path, profit_list, initial_time = 0, discounting_factor = 1/1.15):\n",
    "    #Let only one sample path be given. We compute the CLV.\n",
    "    clv = 0\n",
    "    for t in range(len(path)):\n",
    "        if path[t] != -1: #i.e. the client does not quit\n",
    "            clv += discounting_factor**(initial_time + 1 + t)*profit_list[path[t]]\n",
    "    return clv\n",
    "\n",
    "def CLV_estimation(profit_list, initial_state, transition_matrix, initial_time = 0, discounting_factor = 1/1.15, number_of_paths = 10**4):\n",
    "    sample_paths = generate_sample_paths(number_of_paths, transition_matrix, initial_state, years_limit = 100, client_quit_rate = 0.15)\n",
    "    clv_samples = np.array([CLV_one_path(path, profit_list, initial_time, discounting_factor) for path in sample_paths])\n",
    "    return np.mean(clv_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-52.30311094060062, -111.97371769747686, -153.03238966037094, -128.36185621470375, -134.19660295536264, -107.38918019300343, -172.0544505195912, -52.66559071316109, -115.03707743786597, -148.11506356337898, -151.80545580093835, -124.0903848525705, -123.74424250154775, -154.50847284541334, -40.31285467508943, -44.356708778754225, -32.720055709241294, -64.16265844245274, -184.85929231189812, -104.38543767294786, -82.35595806754266, -63.37123980050352, -96.00886112698804, -166.85490448412847, -80.06965855220443, -44.67540289180584, -81.89498427841893, -38.69915403489263, -133.3907083433842, -172.59523825331348, -94.82695271882065, -100.08263336008108, -145.6402733117331, -182.39635147432213, -139.65304950967567, -200.4667188523514, -223.0120791529179, -191.09923799344523, -152.57707421477582, -82.04866443518664, -176.69423265711018, -153.34165774343734, -97.73342725339367, -76.27468452309603, -76.90304461432035, -102.35513616223892, -66.0043539294268, -192.97196380805642, -224.6020579364584, -132.3626871331242, -126.20236124930175, -166.93463820575272, 17.229052242848027, -94.77901169316424, -60.84762967075551, -83.51132869186048, -62.93026549425807, -61.642154316033896, -57.0014711220497, -124.60324724574045, -11.604319077499706, -9.123945930091711, -33.77402438309031, -35.640941570662065, -24.665557845241814, -36.20940706882503, -24.56219202597527, -18.650778970346256, -123.1335166646463, -147.31834065104857, -99.38959279129924, -122.05249562196809, -214.479104387072, -85.31574711833767, -74.79361472572589, -146.47609723493582, -78.58872607466034, -75.45718008064343, -59.7136238299923, -104.71507704320841, -122.2328721021642, -104.98630923914072, -49.53902101233869, -36.806820310779, -63.25555293207791, 11.876422309625518, -26.439559564957495, -52.58885850963905, 18.45479875864238, 27.90610889709932, -37.873485731527175, -0.5533765615331475, -58.613952091004634, -38.55753778085157, -23.025682289794275, -101.14840571811126, -105.35861853427589, -50.23483531998945, -96.37312454260362, 27.002633874721027, -27.605038081158984, -75.14166170580015, -68.91209192943344, -46.29865246107412, -96.68943260177653, -84.72457579624542, -165.84576051037155, -67.019230548496, -27.10864372385741, -56.032298631206835, -44.09721131037985, -25.590515151154747, -17.720368836737258, -66.77754172786938, -88.74380587353922, -42.35419750259287, -60.5218221097608, -48.28666457126751, 14.147015899619225, -50.9874611633133, 23.924985903408196, -52.737583979358234, -104.93502685611561, 7.476816573800333, 1.9201347891419434, 32.95456467341752, -46.75063125521126, -48.93122419738577, 0.31382420623341434, 5.713350240947989, -42.144401564846206, -75.59187555193994, -121.36018109408974, -131.37467877277052, -96.57261163844753, -225.54972401728438, -89.32611628435066, -40.842058997572465, -54.166949952596646, -73.25872898311937, -113.04440007358664, -66.3134093799246, -76.40157722848504, -69.97955852996222, -89.25045752140916, -35.936682082149844, -43.39462016340504, -11.995358425842046, -14.069791292518792, -38.39198719551218, -115.21940876416598, -9.312127981580463, -78.48333787281987, -100.54077469328499, -13.897376502372168, 11.609306125730715, -9.742951402602287, -73.46229070498266, 32.77779523389505, -20.459051179675136, -38.095927158301585, 30.251047471366878, -8.754476150411026, 56.991800751674376, -22.611904599309714, 30.10639195107617, -0.783057391399835, -12.637026562640738, 23.097241413360884, 0.10770562744581014, -27.583698325668905, -70.92021325609016, 2.6627243881395684, -28.502223138830345, -116.9560409889835, -55.563122505830165, -0.16266876534186422, -18.31739656939511, -21.37193184448549, 8.652981204642789, -36.09686141563926, 27.399948127271188, 6.900599376920155, -31.00353701013095, -32.278228717192846, -2.2224545345734272, 48.12189932287545, 81.48404266321747, 58.7595618402362, -62.427039330739525, 39.91507024029232, 38.60146861714234, 7.001700252317332, -51.2721509561304, -42.429505471526596, 16.06413730706397, 35.304688667288644, 20.0329927536344, 55.87538510628083, 72.57063978576147, -24.8670319215661, -63.52007104212201, -27.265497796612387, 0.058322534687545155, -49.840054469791596, 6.164522041467522, 66.47879567902218, -3.667167630053163, 18.54512792922306, 62.21362001129453, -14.681944590090506, 24.701197839808216, -36.357758811730335, -6.441880984936114, -47.77372114191352, 62.223108346523155, 41.30257392984104, -8.960618496707912, 17.684451114943055, 34.40096131616931, -43.73935138675837, -9.12786708032634, -9.548226987499714, 21.414846091210286, 62.11388135244126, -10.730532852212113, -11.844763740218909, -7.484278442329499, 25.681109052005247, -18.060979004940354, -9.853193366072503, -96.61966196139154, -31.093175818014743, -28.021354019644427, -31.15178089033991, 29.753854149958993, 5.879198886168952, 5.880998497252297, -80.34598796885012, -50.72238539255071, -7.948157595578948, 5.482341760445079, 12.2461640353962, 48.290758148685015, 36.61235800721371, 66.75545642501908, 42.71208993730265, 26.95961616284104, 50.95226935683288, -11.109281419390886, -0.6455048784702214, -8.992444247782366, 57.3852615519512, 20.46060361422338, -59.039818130120864, -31.4855570011619, -37.52126241115121, 15.871154951119001, 63.634448588967686, 29.056812284426723, 43.560399129185775, 42.968362598053666, 27.677864713191376, 78.30634389629631, 85.11595584478492, 76.90060935685862, 67.31425164887037, 110.80373650206873, 103.16678121325995, 21.84268360596743, 10.98319376520992, -5.146014337778471, 20.15079146005275, 81.96118569520418, 16.9909081997666, 46.23540272029601, -5.830181345999742, 58.260520530314196, 50.53993378551033, 87.72164076414396, 4.959499270155378, 15.829454940612345, 53.68343859413932, 59.951529319669035, 24.430712658565177, 9.046042202258816, -24.791574627321815, -40.4556400414764, -28.89071882575098, -6.42899568514876, 4.02732225189317, 17.034835688949595, 11.762664894261976, 42.88535279177383, 23.934554455585392, 43.1673506680324, 55.85346585340987, -0.9965006591340675, 19.343175015685954, 11.244321528588022, 21.064664105441796, 31.359047004140656, 45.84815383227544, 27.118767996918177, 22.14001822348114, 37.184339272834336, 67.01029429839133, 73.50416591222687, 76.2405345273704, 44.14179783062091, 87.46434010577738, 39.26102283876752, 38.62561987039551, 72.08549891879755, 56.69336938069424, -0.12194102778473308, 0.654016043800447, 78.9173882117498, 34.4099090803698, 75.92692445864984, 93.37369098707897, 8.605976839644178, 30.548206040109175, 37.224111965559295, 31.007545998933722, 56.57169390735011, 52.076041986522625, 61.26617854874824, 25.165519177805727, 45.65089697390532, 66.52001625664005, 79.00176028706049, 38.51964333737516, 85.39115867429803, 72.20879208853614, 64.77009319028977, 55.365109693798715, 2.5142506591348606, 80.51222194961801, 28.900825827938082, 61.33037527648979, 55.048782557194855, 99.66533640311883, 33.64449720444658, 103.13975879708093, 88.57868520961321, 70.31272206683873, 97.50299602191187, 68.79448589656893, 50.7305415085833, 58.9022750481328, 32.41585580349366, 69.55333614290467, 43.45026138607612, 77.11550188873457, 59.222201667848275, 121.05334227558899, 81.73000572665106, 85.63899396582002, 108.49615534560499, 98.99041493667303, 46.15796167951787, 42.41673691411049, 58.112244969534075, 60.0556932312694, 68.8153822771804, 72.47275723522415, 109.74167424097477, 80.68685829652173, 59.314427507894756, 99.4984893880613, 120.37325228280618, 83.37504068674373, 135.14067713750333, 109.08504936941827, 99.8596392748911, 82.11093998005083, 68.39067208816172, 85.4392170523257, 49.20360918582837, 78.437724087168, 63.025315710008286, 105.42275683208939, 108.88274257831918, 147.48220162734455, 104.74399044964551, 55.617627934478456, 16.142502480054574, 94.4313908020948, 98.4617128808722, 23.750779442592233, 94.62844249543161, 102.58809386677135, 78.82731248782922, 156.61358530657384, 108.21617181445633, 65.66821190825101, 110.83755052950883, 105.6549392614759, 93.64269710811138, 133.65869552945418, 69.45384689368646, 84.32838193592042, 116.46115177607749, 95.94281048252067, 115.68192385994413, 148.32551259339715, 56.20592919686867, 145.1737090300169, 113.07058387795874, 114.45067557707705, 109.29878647099274, 104.73898325871349, 147.4314140707075, 197.73544958616804, 122.32508099403529, 143.39171762544598, 124.15877923800349, 141.37033514277604, 144.8994590705234, 118.85756384962698, 121.76998456696522, 127.58814059707065, 120.51737458698551, 128.4053063002553, 136.44195712721802, 61.24698075854792, 112.0958248811226]\n"
     ]
    }
   ],
   "source": [
    "CLV_0 = []\n",
    "for i in range(len(states)):\n",
    "    CLV_0.append(CLV_estimation(profit_list = get_profit_list(best_regressor), initial_state= i,\n",
    "transition_matrix = transition_matrix, initial_time= 0, discounting_factor = 1/1.15, number_of_paths = 10))\n",
    "\n",
    "print(CLV_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prac_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
