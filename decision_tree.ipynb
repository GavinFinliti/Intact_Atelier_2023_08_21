{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gen.models import Models\n",
    "from data_gen.models_dict_v2 import model_dict\n",
    "from data_gen.generate_synthetic_df import generate_synthetic_df\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = np.random.seed(1)\n",
    "number_of_samples = 5000\n",
    "train_ratio, valiation_ratio, test_ratio = 0.6,0.2,0.2 #i.e. 0.6*number_of_samples for training, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataframe\n",
    "df = generate_synthetic_df(number_of_samples, SEED)\n",
    "\n",
    "# Instantiate an object from the class \"Models\"\n",
    "models = Models(model_dict)\n",
    "\n",
    "# Calculate the cost and price\n",
    "cost = models.calculate_cost(df)\n",
    "pricing = models.calculate_pricing(df)\n",
    "\n",
    "# Calculate the profit on the synthetic dataframe\n",
    "df[\"profit\"] = models.calculate_profit(cost, pricing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_one_column(column):\n",
    "    mean = np.mean(column)\n",
    "    standard_deviation = np.std(column)\n",
    "    return (column - mean)/standard_deviation\n",
    "\n",
    "def standardize(dataframe):\n",
    "    #Given a pandas dataframe, we standardize every column.\n",
    "    number_of_columns = len(dataframe.columns)\n",
    "    for j in range(number_of_columns):\n",
    "        dataframe.iloc[:,j] = standardize_one_column(dataframe.iloc[:,j])\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'profit']\n",
    "Y = df[\"profit\"]\n",
    "\n",
    "X.loc[X[\"MARITAL_STATUS\"]==\"Single\", \"MARITAL_STATUS\"] = 0\n",
    "X.loc[X[\"MARITAL_STATUS\"]==\"Not_Single\", \"MARITAL_STATUS\"] = 0\n",
    "\n",
    "X = X.astype(int)\n",
    "Y = Y.astype(int)\n",
    "\n",
    "X_TRAIN = X.iloc[:int(train_ratio*number_of_samples),:]\n",
    "Y_TRAIN = Y.iloc[:int(train_ratio*number_of_samples)]\n",
    "X_VALIDATION = X.iloc[int(train_ratio*number_of_samples):int((train_ratio+valiation_ratio)*number_of_samples),:]\n",
    "Y_VALIDATION = Y.iloc[int(train_ratio*number_of_samples):int((train_ratio+valiation_ratio)*number_of_samples)]\n",
    "X_TEST = X.iloc[int((train_ratio+valiation_ratio)*number_of_samples):,:]\n",
    "Y_TEST = Y.iloc[int((train_ratio+valiation_ratio)*number_of_samples):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_hat):\n",
    "    return np.sum( np.square(y-y_hat) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "497937.87520615227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "min_error = 999999999999999999999\n",
    "best_depth = None\n",
    "best_regressor = None\n",
    "\n",
    "for k in range(1,15):\n",
    "    regressor = DecisionTreeRegressor(criterion=\"squared_error\", max_depth=k)\n",
    "    regressor = regressor.fit(X_TRAIN, Y_TRAIN)\n",
    "    Y_VALIDATION_HAT = regressor.predict(X_VALIDATION)\n",
    "    error = MSE(Y_VALIDATION, Y_VALIDATION_HAT)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_depth = k\n",
    "        best_regressor = regressor\n",
    "\n",
    "print(k)\n",
    "print(min_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611439.1069543249\n"
     ]
    }
   ],
   "source": [
    "Y_TEST_HAT = best_regressor.predict(X_TEST)\n",
    "error = MSE(Y_TEST_HAT, Y_TEST)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Get the Transition Matrix\n",
    "\n",
    "By a **state**, we mean a leaf in the best regressor trained above.\n",
    "Our first goal is to get a list of all the states and get the decision path (i.e. given X, what leaf does X fall into?).\n",
    "\n",
    "It turns out that sklearn has one id associated to each tree node. Our `state` will thus be a list of integers corresponding to these leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False  True  True False  True  True\n",
      " False False  True  True False  True  True False False False  True  True\n",
      " False  True  True False False  True  True False  True  True False False\n",
      " False False  True  True False  True  True False False  True  True False\n",
      "  True  True False False False  True  True  True False False  True  True\n",
      "  True False False False False False  True  True False  True  True False\n",
      " False  True  True False  True  True False False False  True  True False\n",
      "  True  True False False  True  True False  True  True False False False\n",
      " False  True  True False  True  True False False  True  True False  True\n",
      "  True False False False  True  True False  True  True False False  True\n",
      "  True False  True  True False False False False False False  True  True\n",
      " False  True  True False False  True  True  True False False False  True\n",
      "  True False  True  True False False  True  True False  True  True False\n",
      " False False False  True  True False  True  True False False  True  True\n",
      " False  True  True False False False  True  True False  True  True False\n",
      " False  True  True False  True  True False False False False False  True\n",
      "  True False  True  True False False  True  True  True False False False\n",
      "  True  True False  True  True False False  True  True False  True  True\n",
      " False False False False  True  True False  True  True False False  True\n",
      "  True False  True  True False False False  True  True False  True  True\n",
      " False False  True  True  True]\n",
      "[  7   8  10  11  14  15  17  18  22  23  25  26  29  30  32  33  38  39\n",
      "  41  42  45  46  48  49  53  54  55  58  59  60  66  67  69  70  73  74\n",
      "  76  77  81  82  84  85  88  89  91  92  97  98 100 101 104 105 107 108\n",
      " 112 113 115 116 119 120 122 123 130 131 133 134 137 138 139 143 144 146\n",
      " 147 150 151 153 154 159 160 162 163 166 167 169 170 174 175 177 178 181\n",
      " 182 184 185 191 192 194 195 198 199 200 204 205 207 208 211 212 214 215\n",
      " 220 221 223 224 227 228 230 231 235 236 238 239 242 243 244]\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "def get_state(regressor):\n",
    "    #Source: https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "    #This method takes our regressor as input and returns an array of boolean plus an array of integers.\n",
    "    #The array of booleans will have the length same as the total number of nodes and will indicate if each is a leaf.\n",
    "    #The array of integers will have the same length as the total number of leaves and will correspond to the location of \"True\" in the boolean array.\n",
    "    \n",
    "    regressor_tree = regressor.tree_\n",
    "\n",
    "    n_nodes = regressor_tree.node_count\n",
    "    children_left = regressor_tree.children_left\n",
    "    children_right = regressor_tree.children_right\n",
    "\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "    while len(stack) > 0:\n",
    "        # `pop` ensures each node is only visited once\n",
    "        node_id, depth = stack.pop()\n",
    "        node_depth[node_id] = depth\n",
    "\n",
    "        # If the left and right child of a node is not the same we have a split\n",
    "        # node\n",
    "        is_split_node = children_left[node_id] != children_right[node_id]\n",
    "        # If a split node, append left and right children and depth to `stack`\n",
    "        # so we can loop through them\n",
    "        if is_split_node:\n",
    "            stack.append((children_left[node_id], depth + 1))\n",
    "            stack.append((children_right[node_id], depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "    \n",
    "    print(is_leaves)\n",
    "    return is_leaves, np.nonzero(is_leaves)[0]\n",
    "\n",
    "is_leaves, states = get_state(best_regressor)\n",
    "print(states)\n",
    "print(len(states))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us print, say, the decision path for `X_TEST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144  76  41  74 238 162  67 178  22  74 170  89 175 160 146 123  46 144\n",
      " 144 115 147 191  89 194  76 100 107 162 192 244 162 175 169 181 170 192\n",
      "  85 162 174 235  77 205  10 112 159  85  81  81  85 192 160  97 131  67\n",
      "  89 138 236 167  66 205 214  81 178 133  69 144  66 130  67 115 174  66\n",
      " 112  67 130 115 115 115  67 159 177 144 144 182  42 174 143 194  41 107\n",
      " 170 224 100 144 230 191  59  74 191  46 175 174 105 207 192 195 178 115\n",
      "  73 144 204 162 144 204 208 112 177  59  73 143  15  89  77 184  53 182\n",
      " 204 235 174  66 115  67 182 100 221 160 162 131 177 178 208 159 120 174\n",
      " 221 162 215 235  60 170 221 119 137  39 175 162 162  84 130 160  74 163\n",
      " 195 191 115 101 112 182  74 177 178  49 100  17  69 170  98  48  67 143\n",
      " 195 144  49 130 131 191  66 130 160 160  59 162  42 143  89 107  25 150\n",
      "  59 207 204 112 221 221 167  73 160 221 184 112 194  73 162 130 182 159\n",
      " 230 194 144 144 185 138 101 130  74 144 143 115  66  41 192 104 191  46\n",
      " 211 144 194 159 160 131 181 144 163 235  74  81 174 107 115 100 100 184\n",
      " 191 130 159 221  74 120 167  77 112 147 130 104 123 175 143 175  89  77\n",
      "  73 159  73 144  67  17  76  15 134 107 184  74 162 214  89 144 174 227\n",
      " 131  25  74 130 177 107  14  74  74  66 184 174 101  81 100  41 211 174\n",
      " 192  66  97  77  66  66 107  45  73  97 144  46 191 174 177 208 133 166\n",
      " 147 112  77 131  41  26  14 177  41 177 215 138  81  39  73 130 131 236\n",
      " 192  97 228 112  67 159  59 205  66  49 159 175 204 123  81 220 131  91\n",
      "  98 177 235 167  74  74  66 143 101 174  73  74 144 163  73 130 235  81\n",
      " 184 107 192 174 112 191  85  18 166 107 178 166  42 214 137  10  54 162\n",
      " 162  74 207 214  67 131 130  73 101 195 235 205 107 194  66 100 105  88\n",
      " 100 100 170 115 137 194  81 146  73  53 160  25 144 131 154  69  74 131\n",
      " 138  15 181 138 199 178  84 144  81  73 100 112 162 100  66 177  77 194\n",
      "  15 181 182 143 144  15 120 143 150  74 235 162 199 181 212 144  15  74\n",
      " 112  98 194  66  97 163 100 160 169  98 143  76 115  97 131 130  74 159\n",
      " 211  15 112 220 137 119  69 211 166 153 146  67 150 235 214 100  67  73\n",
      " 191 144 174  42 169 112 162  69  73 144 147 175 144 150  89 177 175 100\n",
      " 174 192  77 150 184 112 199 112  97  74 175 153 220 119 177  91 130 191\n",
      " 108 100  89 131  15 160  59 177 134 204 100 138 207 115  73 143 194  15\n",
      "  15 100 194 238  46 119 120 167 238 235 174  46 150  73 212  58 115 130\n",
      " 144 207  15 160 151 175  15  59 215 146 104 194  97 166 238  74  25  84\n",
      " 175 166 174 154 195 159  84  97  15 100 159 163 163 144 130 160  77 163\n",
      " 107 162 100  89 199 100  77 144 174 130 230  89  85  97 162  15 146  92\n",
      "  38 100 133  69  77 131 221  98 235  73 220 100  59 205 221  42 131  97\n",
      " 151 163 130 207 162 192 104  85  66 174  76 191 194 177 147 100 177 162\n",
      "  46 100  49 100  92 107 162 137 174 204  98 115  81  66 194  89 235  66\n",
      " 112 182 130 162 181 146  69  97 230 178 214 100  67  92 185 112  91  67\n",
      "  42 115 138  69 205 174 227 101 130 177  67 174  67 169 231  73 178 177\n",
      " 133  98 138 204 143  42  74 235 100 131 131  76  67 162 131 100  26  73\n",
      "  59 115 160 174 101  74  10 144 174 143 191  85 150 112  73 100 184  77\n",
      " 184  74 138  89 147 174  69  81  66 138  58 143 195  76 174  73 223 137\n",
      " 144   7 195 221 191 162 177  15 177 104  89 184  85  67 184  29  42  59\n",
      "  66 163 200  15 100 199 242  74  38 182 204 131  92 160 153  69 143 100\n",
      " 195  98 120  42 175 131 100  58 100  15  74 115 181  73  10  67 163  66\n",
      "  89 100  81  82 137 184 194  88  74  67  66 100 191 191 112  73 195 159\n",
      "  91 151 108 175 211 115 227 159 162 231 144 123 178  67 105  76 143 115\n",
      "  91 144 162  74  14 235  32 130 100 235 144 101 160 131 221 100  41  77\n",
      " 162 214  58 194 170 214 144 230 133 178 182 160 144 107 137 204  15  10\n",
      " 207 227  74 238 236 182 153 182  98 163 191 160 174 159  74 191  77  77\n",
      " 100 177  67 211 130 131 120 131  89  73 146  81 163  73 191  59 235  66\n",
      "  74 162 184 191  67  59 159 130 131 192  25 100 238 130 134 169 143 137\n",
      " 144  77 177 100 238  67  69 184  74 100 160 205 130  77  73   7 100 131\n",
      " 162  81 138  23 133  89 105 131 191 159 153 204  74 175 115 177 199  97\n",
      " 235 175 131 230  15  66 151  73  25  92]\n",
      "[ 70  36  18  35 118  79  31  88   8  35  84  43  86  78  71  61  21  70\n",
      "  70  56  72  93  43  95  36  48  52  79  94 122  79  86  83  89  84  94\n",
      "  41  79  85 116  37 101   2  54  77  41  38  38  41  94  78  46  63  31\n",
      "  43  67 117  82  30 101 106  38  88  64  32  70  30  62  31  56  85  30\n",
      "  54  31  62  56  56  56  31  77  87  70  70  90  19  85  69  95  18  52\n",
      "  84 111  48  70 114  93  28  35  93  21  86  85  51 102  94  96  88  56\n",
      "  34  70 100  79  70 100 103  54  87  28  34  69   5  43  37  91  24  90\n",
      " 100 116  85  30  56  31  90  48 109  78  79  63  87  88 103  77  59  85\n",
      " 109  79 107 116  29  84 109  58  66  17  86  79  79  40  62  78  35  80\n",
      "  96  93  56  49  54  90  35  87  88  23  48   6  32  84  47  22  31  69\n",
      "  96  70  23  62  63  93  30  62  78  78  28  79  19  69  43  52  10  73\n",
      "  28 102 100  54 109 109  82  34  78 109  91  54  95  34  79  62  90  77\n",
      " 114  95  70  70  92  67  49  62  35  70  69  56  30  18  94  50  93  21\n",
      " 104  70  95  77  78  63  89  70  80 116  35  38  85  52  56  48  48  91\n",
      "  93  62  77 109  35  59  82  37  54  72  62  50  61  86  69  86  43  37\n",
      "  34  77  34  70  31   6  36   5  65  52  91  35  79 106  43  70  85 112\n",
      "  63  10  35  62  87  52   4  35  35  30  91  85  49  38  48  18 104  85\n",
      "  94  30  46  37  30  30  52  20  34  46  70  21  93  85  87 103  64  81\n",
      "  72  54  37  63  18  11   4  87  18  87 107  67  38  17  34  62  63 117\n",
      "  94  46 113  54  31  77  28 101  30  23  77  86 100  61  38 108  63  44\n",
      "  47  87 116  82  35  35  30  69  49  85  34  35  70  80  34  62 116  38\n",
      "  91  52  94  85  54  93  41   7  81  52  88  81  19 106  66   2  25  79\n",
      "  79  35 102 106  31  63  62  34  49  96 116 101  52  95  30  48  51  42\n",
      "  48  48  84  56  66  95  38  71  34  24  78  10  70  63  76  32  35  63\n",
      "  67   5  89  67  98  88  40  70  38  34  48  54  79  48  30  87  37  95\n",
      "   5  89  90  69  70   5  59  69  73  35 116  79  98  89 105  70   5  35\n",
      "  54  47  95  30  46  80  48  78  83  47  69  36  56  46  63  62  35  77\n",
      " 104   5  54 108  66  58  32 104  81  75  71  31  73 116 106  48  31  34\n",
      "  93  70  85  19  83  54  79  32  34  70  72  86  70  73  43  87  86  48\n",
      "  85  94  37  73  91  54  98  54  46  35  86  75 108  58  87  44  62  93\n",
      "  53  48  43  63   5  78  28  87  65 100  48  67 102  56  34  69  95   5\n",
      "   5  48  95 118  21  58  59  82 118 116  85  21  73  34 105  27  56  62\n",
      "  70 102   5  78  74  86   5  28 107  71  50  95  46  81 118  35  10  40\n",
      "  86  81  85  76  96  77  40  46   5  48  77  80  80  70  62  78  37  80\n",
      "  52  79  48  43  98  48  37  70  85  62 114  43  41  46  79   5  71  45\n",
      "  16  48  64  32  37  63 109  47 116  34 108  48  28 101 109  19  63  46\n",
      "  74  80  62 102  79  94  50  41  30  85  36  93  95  87  72  48  87  79\n",
      "  21  48  23  48  45  52  79  66  85 100  47  56  38  30  95  43 116  30\n",
      "  54  90  62  79  89  71  32  46 114  88 106  48  31  45  92  54  44  31\n",
      "  19  56  67  32 101  85 112  49  62  87  31  85  31  83 115  34  88  87\n",
      "  64  47  67 100  69  19  35 116  48  63  63  36  31  79  63  48  11  34\n",
      "  28  56  78  85  49  35   2  70  85  69  93  41  73  54  34  48  91  37\n",
      "  91  35  67  43  72  85  32  38  30  67  27  69  96  36  85  34 110  66\n",
      "  70   0  96 109  93  79  87   5  87  50  43  91  41  31  91  12  19  28\n",
      "  30  80  99   5  48  98 120  35  16  90 100  63  45  78  75  32  69  48\n",
      "  96  47  59  19  86  63  48  27  48   5  35  56  89  34   2  31  80  30\n",
      "  43  48  38  39  66  91  95  42  35  31  30  48  93  93  54  34  96  77\n",
      "  44  74  53  86 104  56 112  77  79 115  70  61  88  31  51  36  69  56\n",
      "  44  70  79  35   4 116  14  62  48 116  70  49  78  63 109  48  18  37\n",
      "  79 106  27  95  84 106  70 114  64  88  90  78  70  52  66 100   5   2\n",
      " 102 112  35 118 117  90  75  90  47  80  93  78  85  77  35  93  37  37\n",
      "  48  87  31 104  62  63  59  63  43  34  71  38  80  34  93  28 116  30\n",
      "  35  79  91  93  31  28  77  62  63  94  10  48 118  62  65  83  69  66\n",
      "  70  37  87  48 118  31  32  91  35  48  78 101  62  37  34   0  48  63\n",
      "  79  38  67   9  64  43  51  63  93  77  75 100  35  86  56  87  98  46\n",
      " 116  86  63 114   5  30  74  34  10  45]\n"
     ]
    }
   ],
   "source": [
    "x_test_leaf_id = best_regressor.apply(X_TEST)\n",
    "print(x_test_leaf_id)\n",
    "#print(np.where(states == x_test_leaf_id[0])[0][0])\n",
    "X_TEST_STATES = np.empty(len(X_TEST), dtype = np.uint16)\n",
    "for k in range(len(X_TEST_STATES)):\n",
    "    X_TEST_STATES[k] = np.where(states == x_test_leaf_id[k])[0][0]\n",
    "\n",
    "print(X_TEST_STATES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prac_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
